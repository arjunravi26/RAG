{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_huggingface.llms import HuggingFaceEndpoint\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import spacy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "max_new_tokens = 8192\n",
    "llm_model = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.4,\n",
    "    task='text-generation',\n",
    "    repetition_penalty=1.03\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"jamescalam/llama-2-arxiv-papers-chunked\"\n",
    "data = load_dataset(path=dataset_name, split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1  1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast, fully parameterizable GPU i...   \n",
       "1  We present a fast, fully parameterizable GPU i...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references  \n",
       "0  20110201         []  \n",
       "1  20110201         []  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = data.to_pandas()\n",
    "documents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['chunk'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(X):\n",
    "    chunk1 = X.apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "    chunk2 = chunk1.apply(lambda x: re.sub(r'[\\x00-\\x1F\\x7F]', ' ', x))\n",
    "    chunk3 = chunk2.apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "    chunk4 = chunk3.apply(lambda x: re.sub(r'\\s+([,.!?;:])', r'\\1', x))\n",
    "    chunk5 = chunk4.apply(lambda x: re.sub(r'([,.!?;:])(?=\\S)', r'\\1 ', x))\n",
    "    chunk6 = chunk5.apply(lambda x: re.sub(r'\\{', '-', x))\n",
    "    chunk7 = chunk6.apply(lambda x: re.sub(r'\\}', '', x))\n",
    "    chunk7 = chunk7.apply(lambda x: x.strip())\n",
    "    return chunk7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['chunk'] = preprocess_doc(documents['chunk'])\n",
    "documents['summary'] = preprocess_doc(documents['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4838, 15)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>HighPerformance Neural Networks for Visual Obj...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast fully parameterizable GPU im...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011 Abstract We present a fast fully ...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast fully parameterizable GPU im...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183        0  HighPerformance Neural Networks for Visual Obj...   \n",
       "1  1102.0183        1  January 2011 Abstract We present a fast fully ...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast fully parameterizable GPU im...   \n",
       "1  We present a fast fully parameterizable GPU im...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references  \n",
       "0  20110201         []  \n",
       "1  20110201         []  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "index_name = \"llama-2-rag\"\n",
    "pc = Pinecone(os.getenv('pinecone_api'))\n",
    "pc.delete_index(index_name)\n",
    "index_names = [idx['name'] for idx in pc.list_indexes()]\n",
    "print(index_names)\n",
    "if index_name not in index_names:\n",
    "    pc.create_index(\n",
    "        name=index_name, dimension=384, metric='cosine', spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
    "    timeout = 60\n",
    "    start_time = time.time()\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        if time.time() - start_time >= timeout:\n",
    "            raise TimeoutError(\"Timeout\")\n",
    "        time.sleep(1)\n",
    "pc_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dynamic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_chunking(text, max_token=256, overlap=50):\n",
    "    doc = nlp(text)\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    token_count = 0\n",
    "    for sent in doc:\n",
    "        sent_length = len(tokenizer.tokenize(sent.text))\n",
    "        if sent_length + token_count <= max_token:\n",
    "            current_chunk.append(sent.text)\n",
    "            token_count += sent_length\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                if chunks:\n",
    "                    overlap_text = ' '.join(current_chunk[-overlap:])\n",
    "                    chunks.append(' '.join(current_chunk) + ' ' + overlap_text)\n",
    "                else:\n",
    "                    chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sent.text]\n",
    "            token_count = sent_length\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new data list to store data based on chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dynamically chunking data: 4838it [05:52, 13.71it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data = []\n",
    "m = 0\n",
    "for index, row in tqdm(documents.iterrows(), desc=\"Dynamically chunking data\"):\n",
    "    original_summary = row['chunk']\n",
    "    chunks = dynamic_chunking(original_summary)\n",
    "    n = len(chunks)\n",
    "    m = max(n, m)\n",
    "    for chunk in chunks:\n",
    "        new_row = row.copy()\n",
    "        new_row['chumk'] = chunk\n",
    "        new_row['chunk-id'] = row['chunk-id'] + f\"-{chunks.index(chunk)}\"\n",
    "        new_data.append(new_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe based on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamically_chunked_data = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "      <th>chumk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0-0</td>\n",
       "      <td>HighPerformance Neural Networks for Visual Obj...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast fully parameterizable GPU im...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "      <td>HighPerformance Neural Networks for Visual Obj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1-0</td>\n",
       "      <td>January 2011 Abstract We present a fast fully ...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast fully parameterizable GPU im...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "      <td>January 2011 Abstract We present a fast fully ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         doi chunk-id                                              chunk  \\\n",
       "0  1102.0183      0-0  HighPerformance Neural Networks for Visual Obj...   \n",
       "1  1102.0183      1-0  January 2011 Abstract We present a fast fully ...   \n",
       "\n",
       "          id                                              title  \\\n",
       "0  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1  1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We present a fast fully parameterizable GPU im...   \n",
       "1  We present a fast fully parameterizable GPU im...   \n",
       "\n",
       "                           source  \\\n",
       "0  http://arxiv.org/pdf/1102.0183   \n",
       "1  http://arxiv.org/pdf/1102.0183   \n",
       "\n",
       "                                             authors      categories  \\\n",
       "0  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1  [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "\n",
       "                         comment journal_ref primary_category published  \\\n",
       "0  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1  12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "\n",
       "    updated references                                              chumk  \n",
       "0  20110201         []  HighPerformance Neural Networks for Visual Obj...  \n",
       "1  20110201         []  January 2011 Abstract We present a fast fully ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamically_chunked_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embedding based on GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4899"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status = pc_index.describe_index_stats()\n",
    "status.get(\"total_vector_count\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alredy Created\n"
     ]
    }
   ],
   "source": [
    "status = pc_index.describe_index_stats()\n",
    "if status.get('total_vector_count', 0) == 0:\n",
    "    for i in tqdm(range(0, len(dynamically_chunked_data), batch_size)):\n",
    "        i_end = min(len(dynamically_chunked_data), i + batch_size)\n",
    "        batch = dynamically_chunked_data[i:i_end]\n",
    "        ids = (batch['doi'].astype(str) + '-' +\n",
    "               batch['chunk-id'].astype(str)).to_list()\n",
    "        chunk = batch['chunk'].to_list()\n",
    "        embeds = embeddings.embed_documents(chunk)\n",
    "        meta_data = batch[['chunk', 'source', 'title']\n",
    "                          ].to_dict(orient='records')\n",
    "        pc_index.upsert(vectors=list(zip(ids, embeds, meta_data)))\n",
    "else:\n",
    "    print(\"Alredy Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is LLM?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive relevent text from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(pc_index, embeddings, text_key='chunk')\n",
    "contexts = vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2307.09288-2-0', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='improvements of Llscascmscasc twotaboldstyleChscasctsc in order to enable the community to build on our work and contribute to the responsible development of LLMs Equal contribution corresponding authors tscialom htouvronmetacom ySecond author Contributions for all the authors can be found in Section A1arXiv230709288v2 csCL 19 Jul 2023 Contents 1 Introduction 3 2 Pretraining 5 21 Pretraining Data 5 22 Training Details 5'),\n",
       " Document(id='1806.01261-909-0', metadata={'source': 'http://arxiv.org/pdf/1806.01261', 'title': 'Relational inductive biases, deep learning, and graph networks'}, page_content='m0'),\n",
       " Document(id='2307.09288-285-0', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='13B 4186 4565 9608 34B 4345 4614 967 70B 5018 5337 9621 Finetuned ChatGPT 7846 7992 9853 MPTinstruct 7B 2999 3513 9437 Falconinstruct 7B 2803 4100 8568 Llscascmscasc twotaboldstyleChscasctsc7B 5704 6059 9645 13B 6218 6573 9645 34B 672 7001 9706 70B 6414 6707 9706 Table 44 Evaluation results on TruthfulQA across dierent model generations LimitationsofBenchmarks Itisimportanttonotethattheseevaluationsusingautomaticmetricsareby no means fully comprehensive due to the complex nature of toxicity and bias in LLMs but the benchmarks we selected are representative of our understanding that Llscascmscasc twotaboldstyleChscasctsc improves on critical aspects of LLM')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create augumented prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "You are Arxiv Insight, an expert AI research assistant specialized in analyzing, summarizing, and critiquing research papers from arXiv. Your role is to help users understand complex research by providing clear, step-by-step breakdowns and detailed insights. Follow these guidelines when generating your responses:\n",
    "\n",
    "1. **Context Awareness:**  \n",
    "   Always consider the entire conversation history and the user's query. Leverage this context to ensure your responses are accurate and tailored to the user's needs.\n",
    "\n",
    "2. **Clarification of Request:**  \n",
    "   If the user's query is ambiguous (e.g., asking for a summary, critique, or detailed analysis), ask clarifying questions to confirm their intent before proceeding.\n",
    "\n",
    "3. **Detailed Paper Analysis:**  \n",
    "   - **Summary:** Begin with a concise summary that highlights the paper’s main objectives, methods, and key findings.  \n",
    "   - **Breakdown by Sections:** Explain the paper by dividing it into its key sections (e.g., Introduction, Methodology, Results, Conclusion).  \n",
    "   - **Critical Analysis:** Offer insights on the strengths, limitations, and significance of the research, as well as its context within the broader field.\n",
    "\n",
    "4. **Step-by-Step Explanation:**  \n",
    "   Provide information in logical steps. For example, explain one section of the paper at a time, and ask the user if they need additional details before moving on.\n",
    "\n",
    "5. **Scholarly and Clear Tone:**  \n",
    "   Use precise, academic language while ensuring clarity for users with various levels of expertise. Define technical terms as needed and avoid unnecessary jargon.\n",
    "\n",
    "6. **Interactive Engagement:**  \n",
    "   Conclude your response by inviting further questions or asking follow-up questions to ensure the user’s needs are fully met (e.g., \"Would you like more details on the methodology?\" or \"Do you want a deeper critique of the results?\").\n",
    "\n",
    "**Format for Responses:**\n",
    "\n",
    "**Paper Title:** [Title of the Paper]  \n",
    "- **Summary:**  \n",
    "  [A concise summary of the paper’s objectives, methods, and key findings.]\n",
    "\n",
    "- **Detailed Breakdown:**  \n",
    "  1. **Introduction:**  \n",
    "     [Overview of the research background and objectives.]  \n",
    "  2. **Methodology:**  \n",
    "     [Explanation of the methods, experiments, or theoretical framework used.]  \n",
    "  3. **Results and Discussion:**  \n",
    "     [Summary of findings, interpretations, and implications.]  \n",
    "  4. **Conclusion:**  \n",
    "     [Key takeaways, future directions, and potential impacts.]\n",
    "\n",
    "- **Critical Analysis:** (if applicable)  \n",
    "  - **Strengths:** [Key strengths of the paper.]  \n",
    "  - **Limitations:** [Areas for improvement or potential weaknesses.]  \n",
    "  - **Impact:** [The significance of the research within its field.]\n",
    "\n",
    "**Context:**  \n",
    "{contexts}\n",
    "\n",
    "**User Query:**  \n",
    "{query}\n",
    "\n",
    "**Assistant Response:**  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt = f\"\"\"\n",
    "You are a knowledgeable and friendly AI assistant. Start by warmly greeting the user and sharing an uplifting message.\n",
    "Then, carefully address the user's query by leveraging the provided context, ensuring clarity and depth in your answer.\n",
    "Finally, conclude by asking a thoughtful follow-up question that invites further discussion.\n",
    "Context: {contexts}\n",
    "Query: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are an AI assistent. First Greet the user and give them a positive impact.\\nAnswer below user's query based on the contexts with this prompt and then ask follow up question based on the query and contexts.\\ncontexts: [Document(id='2307.09288-2-0', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='improvements of Llscascmscasc twotaboldstyleChscasctsc in order to enable the community to build on our work and contribute to the responsible development of LLMs Equal contribution corresponding authors tscialom htouvronmetacom ySecond author Contributions for all the authors can be found in Section A1arXiv230709288v2 csCL 19 Jul 2023 Contents 1 Introduction 3 2 Pretraining 5 21 Pretraining Data 5 22 Training Details 5'), Document(id='1806.01261-909-0', metadata={'source': 'http://arxiv.org/pdf/1806.01261', 'title': 'Relational inductive biases, deep learning, and graph networks'}, page_content='m0'), Document(id='2307.09288-285-0', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='13B 4186 4565 9608 34B 4345 4614 967 70B 5018 5337 9621 Finetuned ChatGPT 7846 7992 9853 MPTinstruct 7B 2999 3513 9437 Falconinstruct 7B 2803 4100 8568 Llscascmscasc twotaboldstyleChscasctsc7B 5704 6059 9645 13B 6218 6573 9645 34B 672 7001 9706 70B 6414 6707 9706 Table 44 Evaluation results on TruthfulQA across dierent model generations LimitationsofBenchmarks Itisimportanttonotethattheseevaluationsusingautomaticmetricsareby no means fully comprehensive due to the complex nature of toxicity and bias in LLMs but the benchmarks we selected are representative of our understanding that Llscascmscasc twotaboldstyleChscasctsc improves on critical aspects of LLM')].\\nquery: What is LLM?\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get response from llm using augument prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BroCamp\\Deep Learning\\Chatbot\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Paper Title:** LLaMA: Large Language Model Architecture  \\n- **Summary:** This paper introduces LLaMA, a collection of large language models ranging from 7B to 65B parameters, developed by Meta. The models demonstrate strong performance across various tasks, including code generation, question answering, and common sense reasoning.\\n- **Detailed Breakdown:**\\n  1. **Introduction:** The paper presents LLaMA, a suite of large language models designed to advance the field of natural language processing (NLP) and enable research on large-scale language models. The models are trained on a diverse dataset containing public data up to September 2021.\\n  2. **Model Architecture:** LLaMA models range from 7B to 65B parameters, with a decoder-only transformer architecture. They employ a rotary positional embedding mechanism and use grouped-query attention to improve training efficiency.\\n  3. **Training and Evaluation:** The models are trained using a combination of standard language modeling objectives and instruction tuning. Evaluation results show that LLaMA models outperform or match other state-of-the-art models in various benchmarks, such as MMLU, BBH, and AGI Eval.\\n  4. **Conclusion:** The paper concludes that LLaMA models achieve competitive performance across a wide range of NLP tasks, demonstrating the effectiveness of scaling up language models. The authors release the models to foster further research and innovation in the field.\\n- **Critical Analysis:**\\n  - **Strengths:** LLaMA models exhibit strong performance across diverse tasks, and the paper provides a comprehensive evaluation and analysis of the models\\' capabilities.\\n  - **Limitations:** While the paper demonstrates the benefits of scaling up language models, it does not delve into the environmental impact or ethical considerations of training such large models.\\n  - **Impact:** The release of LLaMA models has contributed to the advancement of large language models and enabled further research in the field of NLP.\\n\\n**Context:** [The user has asked for an explanation of LLM, which is an acronym used in the paper for \"Large Language Model.\"]\\n\\n**Assistant Response:**  \\nIn the context of the provided paper, \"LLM\" stands for \"Large Language Model.\" Throughout the paper, the authors refer to their collection of models, ranging from 7B to 65B parameters, as LLMs. These models are designed to process and generate human-like text based on the input they receive.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BroCamp\\Deep Learning\\Chatbot\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Answer: LLM stands for Large Language Model. It's a type of artificial intelligence model designed to understand and generate human language. These models are trained on vast amounts of text data from the internet and can perform a wide range of tasks, such as answering questions, generating creative content, or even writing code. They're behind many of the chatbots and virtual assistants you interact with online.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.invoke(augmented_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
